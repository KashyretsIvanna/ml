{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Новый раздел"
      ],
      "metadata": {
        "id": "vnA-qoGXYq2f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M38XmUy4aWj_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rftiq_z2a-N5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# 1. Load the Dataset\n",
        "df = pd.read_csv('Reviews.csv')  # Ensure the correct path\n",
        "\n",
        "# 2. Drop Unnecessary Columns\n",
        "df = df.drop(columns=['column_name1', 'column_name2'], errors='ignore')  # Avoid errors if columns are missing\n",
        "\n",
        "# 3. Combine Text Fields\n",
        "df['combined_text'] = df['Summary'].fillna('') + \" \" + df['Text'].fillna('')\n",
        "\n",
        "# 4. Tokenize Text Data\n",
        "tokenizer = Tokenizer(num_words=10_000)\n",
        "tokenizer.fit_on_texts(df['combined_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['combined_text'])\n",
        "\n",
        "# 5. Pad Sequences\n",
        "padded_sequences = pad_sequences(sequences, maxlen=500)\n",
        "\n",
        "# 6. Map Scores (if applicable)\n",
        "df['score'] = df['Score'].map({5: 1, 4: 1, 3: 0, 2: 0, 1: 0})  # Ensure column name is correct\n",
        "\n",
        "# 7. Handle Class Imbalance\n",
        "ros = RandomOverSampler()\n",
        "X_resampled, y_resampled = ros.fit_resample(padded_sequences, df['score'].dropna())\n",
        "\n",
        "# 8. Encode Labels\n",
        "y_categorical = to_categorical(y_resampled, num_classes=2)\n",
        "\n",
        "# 9. Split Data into Training and Validation Sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Preprocessing complete!\")"
      ],
      "metadata": {
        "id": "ZI65goYeYsXY",
        "outputId": "5893513a-bfb5-4f1b-e6aa-0bf098161918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Itt2lH4mbeYG",
        "outputId": "00335f33-d7bc-41ff-c4a7-bc7b7eb0a313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   0    0    0 ...  278   12 7492]\n",
            " [   0    0    0 ...  106 3617  297]\n",
            " [   0    0    0 ...   93  282    6]\n",
            " ...\n",
            " [   0    0    0 ...   90   62  411]\n",
            " [   0    0    0 ... 2135   12    6]\n",
            " [   0    0    0 ...  722    1  650]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm 2 CNN Model for text classification\n"
      ],
      "metadata": {
        "id": "aW_LSkikbxrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n"
      ],
      "metadata": {
        "id": "M6LdqMWAbwR7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler  # Import this line\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1: Load the Dataset\n",
        "df = pd.read_csv('Reviews.csv')  # Ensure the correct path\n",
        "\n",
        "# 2: Drop Unnecessary Columns\n",
        "df = df.drop(columns=['column_name1', 'column_name2'], errors='ignore')\n",
        "\n",
        "# 3: Combine Text Fields\n",
        "df['combined_text'] = df['Summary'].fillna('') + \" \" + df['Text'].fillna('')\n",
        "\n",
        "# 4: Tokenize Text Data\n",
        "vocab_size = 10_000\n",
        "max_sequence_length = 500  # Max length for sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['combined_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['combined_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# 5: Map Scores (Binary Classification)\n",
        "df['score'] = df['Score'].map({5: 1, 4: 1, 3: 0, 2: 0, 1: 0})  # Ensure column is mapped correctly\n",
        "\n",
        "# Drop any rows with missing scores after mapping\n",
        "df = df.dropna(subset=['score'])\n",
        "\n",
        "# 6: Handle Imbalanced Classes\n",
        "ros = RandomUnderSampler(random_state=42)  # Corrected import\n",
        "X_resampled, y_resampled = ros.fit_resample(padded_sequences, df['score'])\n",
        "\n",
        "# 7: Encode Labels for Categorical Crossentropy\n",
        "num_classes = 2\n",
        "y_categorical = to_categorical(y_resampled, num_classes=num_classes)\n",
        "\n",
        "# 8: Split Data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9: Initialize Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(64, 3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))  # num_classes must be set properly\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the Model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate Model\n",
        "loss, acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {acc * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "9k5OVBVhb2PO",
        "outputId": "583e1cbc-ae7c-4f12-f6f8-3811ee324131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5947/6234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1:12\u001b[0m 253ms/step - accuracy: 0.8571 - loss: 0.3147"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C7jpV7aviFEU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7t2H7dUGiEcv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Добро пожаловать в Colaboratory!",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}